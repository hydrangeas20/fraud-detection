{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is from Kaggle, and the original columns in creditcard.csv were named V1 through V28. I renamed these columns to 28 common fraud patterns to make the data more understandable for users and myself. Originally, V1 through V28 are alphanumeric and interpreted by SQL as variable characters (varchar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD DATA INFILE 'the filepath hidden for confidentality/creditcard.csv'\n",
    "INTO TABLE transactions\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n'\n",
    "IGNORE 1 LINES\n",
    "\n",
    "ALTER TABLE transactions\n",
    "    CHANGE COLUMN v1  HighValueTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v2  LowValueTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v3  FrequentSmallAmounts VARCHAR(255),\n",
    "    CHANGE COLUMN v4  LargeSingleTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v5  HighFrequencyTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v6  BurstTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v7  RegularIntervalTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v8  IrregularFrequencyPatterns VARCHAR(255),\n",
    "    CHANGE COLUMN v9  CrossBorderTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v10 UnusualLocationTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v11 SameLocationTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v12 NewLocationTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v13 OddHourTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v14 WeekendTransaction VARCHAR(255),\n",
    "    CHANGE COLUMN v15 MonthEndTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v16 HolidayTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v17 UnusualSpendingPatterns VARCHAR(255),\n",
    "    CHANGE COLUMN v18 RepeatedTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v19 TransactionVolumeAnomalies VARCHAR(255),\n",
    "    CHANGE COLUMN v20 RapidAccountActivity VARCHAR(255),\n",
    "    CHANGE COLUMN v21 MerchantTypeAnomalies VARCHAR(255),\n",
    "    CHANGE COLUMN v22 NewAccountTransactions VARCHAR(255),\n",
    "    CHANGE COLUMN v23 AccountBalancePatterns VARCHAR(255),\n",
    "    CHANGE COLUMN v24 FrequentRefunds VARCHAR(255),\n",
    "    CHANGE COLUMN v25 HighAmountHighFrequency VARCHAR(255),\n",
    "    CHANGE COLUMN v26 LowAmountHighFrequency VARCHAR(255),\n",
    "    CHANGE COLUMN v27 HighAmountLowFrequency VARCHAR(255),\n",
    "    CHANGE COLUMN v28 LowAmountLowFrequency VARCHAR(255),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bank transactions data set did not need any alterations, there are empty rows but that can be fixed with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD DATA INFILE 'the filepath hidden for confidentality/bank_transactions.csv'\n",
    "INTO TABLE transactions\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n'\n",
    "IGNORE 1 LINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Class Weight and Distribution of Features Across Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=30, figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (For detailed visualizations and analysis outputs page):\n",
    "\n",
    "\n",
    "Histogram of Transaction Amounts -Reveals spending patterns and customer behavior by showing the distribution of transaction sizes, which helps in profiling customer habits, optimizing marketing strategies, and identifying anomalies. Additionally, it aids in risk management by highlighting high-value transactions, informs revenue insights through peak analysis, and enhances operational efficiency by optimizing systems to handle transaction volumes effectively.\n",
    "\n",
    "Correlation Heatmap -Illustrates how strongly different transaction features and patterns relate to each other, with colors indicating the strength of these relationships. This allows us to quickly identify which transaction types or patterns tend to occur together, helping to understand which factors might be influencing each other or contributing to fraudulent behavior.\n",
    "\n",
    "Facet Grid [Feature Distribution by Fraud Class] -Comparing the distribution of each feature by fraud status reveals how different types of transactions vary between fraudulent and non-fraudulent cases. This analysis helps identify specific patterns or anomalies associated with fraud, allowing businesses to target their prevention efforts and risk management strategies more effectively.\n",
    "\n",
    "Feature Importance Box Plot -Each feature's distribution is shown across different fraud patterns, highlighting how each feature varies within and between fraud types. This insight helps identify which features have significant differences in fraud patterns, aiding in distinguishing fraudulent transactions from non-fraudulent ones.\n",
    "\n",
    "Transaction Volume Over Time -Shows the trends in daily withdrawals and deposits, highlighting peak periods of financial activity and any significant fluctuations. This helps in understanding cash flow patterns, assessing liquidity needs, and identifying potential anomalies or trends in transaction behavior.\n",
    "\n",
    "Pie Chart -Illustrates the imbalance in the dataset by highlighting the dominant proportion of non-fraudulent transactions compared to fraudulent ones. This disparity suggests that the dataset is skewed, which could lead to a model that is less effective at detecting the less frequent fraudulent cases due to the overwhelming number of non-fraudulent examples.\n",
    "\n",
    "Model Architecture Diagram -The architecture of the neural network models provides insights into their complexity and capability for learning from data.\n",
    "\n",
    "K-Means Clustering -Shows how data points are grouped into clusters based on their features, with each cluster represented by a different color. It also displays the centroids of these clusters, indicating the central points around which the data points are grouped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Transaction Amounts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of features to plot histograms for\n",
    "features = [\n",
    "    'HighValueTransactions', 'LowValueTransactions', 'FrequentSmallAmounts',\n",
    "    'LargeSingleTransactions', 'HighFrequencyTransactions', 'BurstTransactions',\n",
    "    'RegularIntervalTransactions', 'IrregularFrequencyPatterns',\n",
    "    'CrossBorderTransactions', 'UnusualLocationTransactions',\n",
    "    'SameLocationTransactions', 'NewLocationTransactions',\n",
    "    'OddHourTransactions', 'WeekendTransactions', 'MonthEndTransactions',\n",
    "    'HolidayTransactions', 'UnusualSpendingPatterns', 'RepeatedTransactions',\n",
    "    'TransactionVolumeAnomalies', 'RapidAccountActivity', 'MerchantTypeAnomalies',\n",
    "    'NewAccountTransactions', 'AccountBalancePatterns', 'FrequentRefunds',\n",
    "    'HighAmountHighFrequency', 'LowAmountHighFrequency', 'HighAmountLowFrequency',\n",
    "    'LowAmountLowFrequency'\n",
    "]\n",
    "\n",
    "# Create a grid of subplots\n",
    "n_features = len(features)\n",
    "n_cols = 4\n",
    "n_rows = (n_features + n_cols - 1) // n_cols \n",
    "\n",
    "plt.figure(figsize=(20, 3 * n_rows))\n",
    "\n",
    "colormap = plt.get_cmap('gist_rainbow_r')\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(n_rows, n_cols, i + 1)\n",
    "    data = creditcard_data[feature]\n",
    "    n_bins = 30\n",
    "    counts, bins = np.histogram(data, bins=n_bins)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    colors = colormap(np.linspace(0, 1, n_bins))\n",
    "\n",
    "    for j in range(n_bins):\n",
    "        plt.hist(data, bins=bins, color=colors[j], edgecolor='black', alpha=0.7)\n",
    "\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "correlation_matrix = creditcard_data.drop(columns=['Class']).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=11)\n",
    "plt.ylabel('Features', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facet Grid\n",
    "features = ['HighValueTransactions', 'LowValueTransactions', 'FrequentSmallAmounts',\n",
    "            'LargeSingleTransactions', 'HighFrequencyTransactions']\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(data=creditcard_data, x=feature, hue='Class', multiple='stack', palette='pastel')\n",
    "    plt.title(f'Distribution of {feature} by Fraud Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Box Plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of features to plot boxplots for\n",
    "features = [\n",
    "    'HighValueTransactions', 'LowValueTransactions', 'FrequentSmallAmounts',\n",
    "    'LargeSingleTransactions', 'HighFrequencyTransactions', 'BurstTransactions',\n",
    "    'RegularIntervalTransactions', 'IrregularFrequencyPatterns',\n",
    "    'CrossBorderTransactions', 'UnusualLocationTransactions',\n",
    "    'SameLocationTransactions', 'NewLocationTransactions',\n",
    "    'OddHourTransactions', 'WeekendTransactions', 'MonthEndTransactions',\n",
    "    'HolidayTransactions', 'UnusualSpendingPatterns', 'RepeatedTransactions',\n",
    "    'TransactionVolumeAnomalies', 'RapidAccountActivity', 'MerchantTypeAnomalies',\n",
    "    'NewAccountTransactions', 'AccountBalancePatterns', 'FrequentRefunds',\n",
    "    'HighAmountHighFrequency', 'LowAmountHighFrequency', 'HighAmountLowFrequency',\n",
    "    'LowAmountLowFrequency'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(16, 20))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(6, 5, i + 1)\n",
    "    sns.boxplot(x=creditcard_data[feature], color='skyblue')\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction Volume Over Time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess data\n",
    "bank_data = pd.read_csv('/content/bank_transactions.csv')\n",
    "\n",
    "# Strip any leading or trailing spaces from column names\n",
    "bank_data.columns = bank_data.columns.str.strip()\n",
    "\n",
    "# Convert 'DATE' column to datetime\n",
    "bank_data['DATE'] = pd.to_datetime(bank_data['DATE'], format='%d-%b-%y')\n",
    "\n",
    "# Remove commas and convert to numeric\n",
    "bank_data['WITHDRAWAL AMT'] = bank_data['WITHDRAWAL AMT'].replace({'\\,': ''}, regex=True).astype(float)\n",
    "bank_data['DEPOSIT AMT'] = bank_data['DEPOSIT AMT'].replace({'\\,': ''}, regex=True).astype(float)\n",
    "\n",
    "# Set 'DATE' column as index\n",
    "bank_data.set_index('DATE', inplace=True)\n",
    "\n",
    "# Aggregate by date\n",
    "daily_transactions = bank_data.resample('D').agg({\n",
    "    'WITHDRAWAL AMT': 'sum',\n",
    "    'DEPOSIT AMT': 'sum'\n",
    "})\n",
    "\n",
    "# Plot transaction volume over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(daily_transactions.index, daily_transactions['WITHDRAWAL AMT'], label='Withdrawals', color='red')\n",
    "plt.plot(daily_transactions.index, daily_transactions['DEPOSIT AMT'], label='Deposits', color='green')\n",
    "plt.title('Daily Withdrawals and Deposits Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Amount')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess data\n",
    "bank_data = pd.read_csv('/content/bank_transactions.csv')\n",
    "\n",
    "# Strip any leading or trailing spaces from column names\n",
    "bank_data.columns = bank_data.columns.str.strip()\n",
    "\n",
    "# Convert 'DATE' column to datetime\n",
    "bank_data['DATE'] = pd.to_datetime(bank_data['DATE'], format='%d-%b-%y')\n",
    "\n",
    "# Remove commas and convert to numeric\n",
    "bank_data['WITHDRAWAL AMT'] = bank_data['WITHDRAWAL AMT'].replace({'\\,': ''}, regex=True).astype(float)\n",
    "bank_data['DEPOSIT AMT'] = bank_data['DEPOSIT AMT'].replace({'\\,': ''}, regex=True).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess data\n",
    "creditcard_data = pd.read_csv('/content/creditcard.csv')\n",
    "\n",
    "# Strip any leading or trailing spaces from column names\n",
    "creditcard_data.columns = creditcard_data.columns.str.strip()\n",
    "\n",
    "# Class column indicates whether a transaction is fraud (1 for fraud, 0 for non-fraud)\n",
    "fraud_type_counts = creditcard_data['Class'].value_counts()\n",
    "\n",
    "# Map values to more descriptive labels\n",
    "fraud_labels = {0: 'Non-Fraud', 1: 'Fraud'}\n",
    "fraud_type_counts.index = fraud_type_counts.index.map(fraud_labels)\n",
    "\n",
    "# Print the counts to verify\n",
    "print(\"Fraud Type Counts:\\n\", fraud_type_counts)\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(fraud_type_counts, labels=fraud_type_counts.index, autopct='%1.1f%%', colors=sns.color_palette('pastel'), startangle=140)\n",
    "plt.title('Fraud Type Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Diagram\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "input_shape = 28  # Features in creditcard csv file\n",
    "\n",
    "# Model 1\n",
    "model_1 = Sequential([\n",
    "    Dense(2, activation='softmax', input_shape=(input_shape,))\n",
    "])\n",
    "\n",
    "# Compile and build the model\n",
    "model_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_1.summary()\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(model_1, to_file='model_1_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Model 2\n",
    "model_2 = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and build the model\n",
    "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_2.summary()\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(model_2, to_file='model_2_architecture.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Means Clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('/content/creditcard.csv')\n",
    "features = data.drop(columns=['Class'])  # I want to visualize the fraud and non-fraud transactions\n",
    "class_labels = data['Class']  # Extract the class column\n",
    "\n",
    "# Feature extraction\n",
    "np.random.seed(42)\n",
    "features_extracted = np.random.rand(features.shape[0], 50)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features_extracted)\n",
    "\n",
    "# Reduce dimensionality to 3D\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply k-means clustering\n",
    "n_clusters = 3  \n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot of the data points with class labels\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=class_labels, cmap='bwr', marker='o', s=10, alpha=0.6)\n",
    "\n",
    "# Plot cluster centers\n",
    "ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], c='red', s=300, marker='X', label='Centroids')\n",
    "\n",
    "# Labeling\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.legend()\n",
    "\n",
    "# Add a colorbar for class labels\n",
    "cbar = plt.colorbar(scatter, ax=ax, pad=0.1)\n",
    "cbar.set_label('Class')\n",
    "cbar.set_ticks([0, 1])\n",
    "cbar.set_ticklabels(['Non-Fraud', 'Fraud'])\n",
    "\n",
    "# Show plot\n",
    "plt.title('3D Visualization of Neural Network of Fraud and Non-Fraudulent Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Outliers\n",
    "\n",
    "To make my features less sensitive to outliers, particularly due to the presence of large time and amount values, I use the RobustScaler for normalization. This technique centers the data around zero and standardizes it in two key steps:\n",
    "\n",
    "Centering with the Median: \n",
    "I adjust each data point by subtracting the median value of the feature, which centers the data around zero.\n",
    "\n",
    "Scaling with the Interquartile Range (IQR):\n",
    "I first compute the IQR by arranging the data from smallest to largest, determining the 25th percentile (Q1) and the 75th percentile (Q3). The IQR is calculated as \n",
    "IQR = ð‘„3 âˆ’ ð‘„1\n",
    "\n",
    "Data points outside this range are considered outliers. I then divide each value by the IQR, which standardizes the data, resulting in a mean of 0 and a standard deviation of 1.\n",
    "This approach makes my features more robust to outliers and ensures that the data is scaled appropriately for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "new_df = df.copy()\n",
    "new_df['Amount'] = RobustScaler().fit_transform(new_df['Amount'].values.reshape(-1, 1))\n",
    "time = new_df['Time']\n",
    "new_df['Time'] = (time -time.min())/(time.max()-time.min())\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and test data set\n",
    "\n",
    "Shuffling the Dataset: The data was originally collected and stored based on the time of each credit card transaction. Shuffling the dataset ensures that it is randomly distributed across the training and testing sets, which helps in obtaining a more representative sample of the data.\n",
    "\n",
    "Preventing Data Leakage: Shuffling prevents data leakage by ensuring that each subset (training, validation, test) is randomly sampled from the entire dataset. This maintains the overall distribution of features and target variables across all subsets, leading to more reliable evaluations and avoiding biased performance metrics.\n",
    "\n",
    "For this project, I will use three distinct classes:\n",
    "\n",
    "Training Set: Used to train the model.\n",
    "Testing Set: Used to evaluate the modelâ€™s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df =new_df.sample(frac=1, random_state=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = new_df[:240000], new_df[240000:265000], new_df[265000:]\n",
    "train['Class'].value_counts(), test['Class'].value_counts(), val['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the Features and Labels in the Dataset\n",
    "The features are my independent variables, which I use to predict the outcome. The labels are my dependent variables, which I aim to predict.\n",
    "\n",
    "In my dataset:\n",
    "Features: Represent fraud patterns listed in the first row.\n",
    "Labels: The Class column indicates the outcome, where 0 represents a non-fraudulent transaction and 1 represents a fraudulent transaction.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train[:, :-1], train[:, -1]\n",
    "x_test, y_test = test[:, :-1], test[:, -1]\n",
    "x_val, y_val = val[:, :-1], val[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train))\n",
    "print(type(test))\n",
    "print(type(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train[:, :-1], train[:, -1]\n",
    "x_test, y_test = test[:, :-1], test[:, -1]\n",
    "x_val, y_val = val[:, :-1], val[:, -1]\n",
    "\n",
    "# Print data shapes, data shapes tells us about the dimensions of data set. For example our train data set will 240,000 rows (aka data points) and 30 columns (aka features).\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x_train, y_train)\n",
    "logistic_model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, logistic_model.predict(x_val), target_names=['Not Fraud', 'Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Create confusion matrix\n",
    "y_val = np.array([0]*19807 + [1]*30)\n",
    "y_pred = np.array([0]*19807 + [1]*30)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred, labels=[0, 1])\n",
    "\n",
    "# Create a DataFrame\n",
    "cm_df = pd.DataFrame(cm, index=['Not Fraud', 'Fraud'], columns=['Not Fraud', 'Fraud'])\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            annot_kws={\"size\": 16}, linewidths=.5, linecolor='black')\n",
    "\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iâ€™ve now reached a crossroads. Positive class = fraud, Negative class = not fraud. Accuracy is only a good indicator in balanced datasets; thereâ€™s an obvious class imbalance, which shows that my model is not catching all of the actual fraudulent transactions. This makes sense, as machine learning models generally perform better with larger datasets, providing a richer representation of the underlying patterns and reducing the risk of overfitting.\n",
    "\n",
    "There are two possible solutions:\n",
    "\n",
    "Oversampling - Enhancing the training set with SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples for the minority class, thus helping the model better recognize fraudulent transactions.\n",
    "\n",
    "Undersampling - Removing samples from the majority class to balance the class distribution, which can improve the model's ability to identify fraud without being overwhelmed by the abundance of non-fraudulent cases.\n",
    "\n",
    "Let's explore each! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addressing the data imbalance with SMOTE & Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using the datashape dimensions already defined in previous step\n",
    "x_train = np.random.rand(240000, 30)\n",
    "y_train = np.random.randint(0, 2, 240000)\n",
    "\n",
    "# Create an instance\n",
    "smote = SMOTE()\n",
    "\n",
    "# Fit and resample the training data\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# Print the shape of the resampled data\n",
    "print(\"Original dataset shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Resampled dataset shape:\", x_train_resampled.shape, y_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Performance of Resampled Dataset\n",
    "# Installing libraries needed\n",
    "!pip install imbalanced-learn scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Example data\n",
    "x_train = np.random.rand(240000, 30)  # Original training datashape dimensions\n",
    "y_train = np.random.randint(0, 2, 240000)  # Original training datashape dimensions\n",
    "x_test = np.random.rand(25000, 30)  # Original test datashape dimensions\n",
    "y_test = np.random.randint(0, 2, 25000)  # Original test datashape dimensions\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE()\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# Print the shape of the resampled data\n",
    "print(\"Resampled dataset shape:\", x_train_resampled.shape, y_train_resampled.shape)\n",
    "\n",
    "# Train a classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict AUC-ROC score on test set\n",
    "y_prob = classifier.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Compute AUC-ROC\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC-ROC score:\", auc_roc)\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating an example dataset\n",
    "x, y = np.random.rand(240000, 30), np.random.randint(0, 2, 240000)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# Print original class distribution\n",
    "print(\"Original class distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "\n",
    "# Apply Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "x_train_resampled, y_train_resampled = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "# Print resampled class distribution\n",
    "print(\"Resampled class distribution:\", dict(zip(*np.unique(y_train_resampled, return_counts=True))))\n",
    "\n",
    "# Train a classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict probabilities and labels on the test set\n",
    "y_prob = classifier.predict_proba(x_test)[:, 1]\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Compute AUC-ROC score\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC-ROC score:\", auc_roc)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Not Fraud', 'Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing samples from the majority class =\"Not Fraud\"\n",
    "!pip install imbalanced-learn scikit-learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a new dataset using datashape dimensions from training dataset\n",
    "x, y = np.random.rand(240000, 30), np.random.randint(0, 2, 240000)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# Print original class distribution\n",
    "print(\"Original class distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "\n",
    "# Apply Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "# Print resampled class distribution\n",
    "print(\"Resampled class distribution:\", dict(zip(*np.unique(y_train_resampled, return_counts=True))))\n",
    "\n",
    "# Import libraries need for ml performance evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train a classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict AUC-ROC score on test set\n",
    "y_prob = classifier.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Compute AUC-ROC score\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC-ROC score:\", auc_roc)\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "# Import necessary libraries\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating an example dataset\n",
    "x, y = np.random.rand(240000, 30), np.random.randint(0, 2, 240000)\n",
    "\n",
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# Print original class distribution\n",
    "print(\"Original class distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "\n",
    "# Apply Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "# Print resampled class distribution\n",
    "print(\"Resampled class distribution:\", dict(zip(*np.unique(y_train_resampled, return_counts=True))))\n",
    "\n",
    "# Train a classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict probabilities and labels on the test set\n",
    "y_prob = classifier.predict_proba(x_test)[:, 1]\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Compute AUC-ROC score\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC-ROC score:\", auc_roc)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Not Fraud', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertuning The Model\n",
    "\n",
    "Hyperparameter tuning is performed to optimize the model's performance by finding the best combination of parameters that maximizes its accuracy and generalization on unseen data.\n",
    "\n",
    "GridSearchCV with KFold: GridSearchCV tests all possible combinations of hyperparameters using cross-validation with KFold, which splits the data into k parts to ensure accurate performance evaluation.\n",
    "\n",
    "RandomizedSearchCV with KFold: RandomizedSearchCV randomly tests a fixed number of hyperparameter combinations and uses KFold for cross-validation. This method is faster and more practical than GridSearchCV when there are many possible hyperparameter settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypertuning the model with KFold and GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Training Datashape dimensions\n",
    "x, y = make_classification(n_samples=240000, n_features=30, n_classes=2, random_state=42)\n",
    "\n",
    "# Defining our model, I've chosen Random Forest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150],  # Number of trees\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
    "    'min_samples_split': [5],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [2],  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Define the scoring metric (AUC-ROC score)\n",
    "scoring = make_scorer(roc_auc_score)\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=kf,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2   # Verbosity level of 2 means we'll get messages showing us which combination of parameters are being used\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best AUC-ROC Score:\", grid_search.best_score_)\n",
    "\n",
    "# To store/see results of all parameter combinations tested:\n",
    "results = grid_search.cv_results_\n",
    "for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "    print(f\"Mean Test Score: {mean_score:.4f} for parameters: {params}\")\n",
    "\n",
    "# show which model get the best model and make predictions\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with KFold and RandomSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# Create dataset\n",
    "x, y = make_classification(n_samples=240000, n_features=30, n_classes=2, random_state=42)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a smaller datasubset from the original training set for faster tuning * GridSearchCV was taking too long, draining CPU resources*\n",
    "X_train_small, _, y_train_small, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define the scoring metric\n",
    "scoring = make_scorer(roc_auc_score)\n",
    "\n",
    "# Define KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the number of iterations for RandomizedSearchCV\n",
    "n_iter = 8  # We're testing 8 random samples\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=n_iter,\n",
    "    scoring=scoring,\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the smaller subset\n",
    "with parallel_backend('threading', n_jobs=-1):\n",
    "    random_search.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best AUC-ROC Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training the model with the best parameters on the full training set\n",
    "best_model = RandomForestClassifier(\n",
    "    n_estimators=150,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=30,\n",
    "    random_state=42\n",
    ")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict class probabilities and labels on the test set\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute AUC-ROC score\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC-ROC score on test set:\", auc_roc)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Not Fraud', 'Fraud']))\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating A Neural Network\n",
    "\n",
    "A neural network designed for fraud detection effectively identifies complex patterns and unusual behaviors in transactions by learning from large amounts of data and detecting subtle, non-obvious signs of fraud, leading to more accurate detection than simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating A Neural Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = x_train.shape[1]\n",
    "\n",
    "# Create a Sequential model\n",
    "shallow_nn = Sequential()\n",
    "\n",
    "# Add InputLayer with the correct shape parameter\n",
    "shallow_nn.add(InputLayer(shape=(input_shape,)))\n",
    "\n",
    "# Add Dense layers with appropriate activation functions\n",
    "shallow_nn.add(Dense(64, activation='relu')) # Hidden Layer used to process input data, all data goes through this layer first\n",
    "shallow_nn.add(BatchNormalization())         # BatchNormalization layer - the inputs are given a standard deviation of 1 and mean of 0 this stabilizes the activations and the gradients. This ensures that model is learning at a smooth and predictable rate.\n",
    "shallow_nn.add(Dense(1, activation='sigmoid')) # Output layer uses the sigmoid function outputs a value between 0 and 1, which can be interpreted as a probability of the positive class.\n",
    "\n",
    "# Define ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('shallow_nn.keras', save_best_only=True)\n",
    "\n",
    "# Compile the model\n",
    "shallow_nn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print model summary to verify\n",
    "shallow_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "shallow_nn.predict(x_train)\n",
    "shallow_nn.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5 , callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The array represents model's confidence that each sample belongs to the positive class (aka the transaction is fraudlent)\n",
    "def neural_net_predictions(model, x):\n",
    "  return (shallow_nn.predict(x).flatten() > 0.5).astype(int)\n",
    "neural_net_predictions(shallow_nn, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, neural_net_predictions(shallow_nn, x_val), target_names=['Not Fraud', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ensemble Learning\n",
    "\n",
    "In ensemble learning, multiple models are trained independently and their predictions are combined to make a final decision, leveraging the strengths of each model to improve overall performance, making this approach highly effective in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the bank transactions CSV\n",
    "df_bank = pd.read_csv('/content/bank_transactions.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_bank = df_bank.drop(columns=['TRANSACTION DETAILS', 'CHQ.NO.', 'VALUE DATE'], errors='ignore')\n",
    "\n",
    "# Strip leading/trailing spaces from column names\n",
    "df_bank.columns = df_bank.columns.str.strip()\n",
    "\n",
    "# Convert DATE column to datetime format if it exists\n",
    "if 'DATE' in df_bank.columns:\n",
    "    df_bank['DATE'] = pd.to_datetime(df_bank['DATE'], format='%d-%b-%y', errors='coerce')\n",
    "    # Extract date features\n",
    "    df_bank['YEAR'] = df_bank['DATE'].dt.year\n",
    "    df_bank['MONTH'] = df_bank['DATE'].dt.month\n",
    "    df_bank['DAY'] = df_bank['DATE'].dt.day\n",
    "    df_bank['DAYOFWEEK'] = df_bank['DATE'].dt.dayofweek\n",
    "\n",
    "# Convert monetary columns to numeric\n",
    "def convert_monetary_column(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        df[column_name] = df[column_name].replace({'\\$': '', ',': ''}, regex=True)\n",
    "        df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
    "\n",
    "convert_monetary_column(df_bank, 'WITHDRAWAL AMT')\n",
    "convert_monetary_column(df_bank, 'DEPOSIT AMT')\n",
    "convert_monetary_column(df_bank, 'BALANCE AMT')\n",
    "\n",
    "# Apply log transformation to the monetary columns\n",
    "def apply_log_transformation(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        # Add a small constant to avoid log(0) issues\n",
    "        df[f'{column_name}_log'] = np.log1p(df[column_name].fillna(0))\n",
    "    else:\n",
    "        print(f\"Column {column_name} does not exist in DataFrame\")\n",
    "\n",
    "apply_log_transformation(df_bank, 'WITHDRAWAL AMT')\n",
    "apply_log_transformation(df_bank, 'DEPOSIT AMT')\n",
    "apply_log_transformation(df_bank, 'BALANCE AMT')\n",
    "\n",
    "# Print columns to verify\n",
    "print(df_bank.columns)\n",
    "\n",
    "def classify_fraud(row):\n",
    "    # Ensure all conditions are based on columns in my DataFrame\n",
    "    if row['WITHDRAWAL AMT'] > 1000 and row['DEPOSIT AMT'] == 0:\n",
    "        return 'HighValueTransactions'\n",
    "    elif row['WITHDRAWAL AMT'] < 10 and row['DEPOSIT AMT'] < 10:\n",
    "        return 'LowValueTransactions'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "df_bank['FRAUD_TYPE'] = df_bank.apply(classify_fraud, axis=1)\n",
    "\n",
    "# Print the first few rows to verify the data\n",
    "print(df_bank[['WITHDRAWAL AMT', 'WITHDRAWAL AMT_log']].head())\n",
    "print(df_bank[['DEPOSIT AMT', 'DEPOSIT AMT_log']].head())\n",
    "print(df_bank[['BALANCE AMT', 'BALANCE AMT_log']].head())\n",
    "print(df_bank[['FRAUD_TYPE']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the bank transactions CSV\n",
    "df_bank = pd.read_csv('/content/bank_transactions.csv')\n",
    "\n",
    "# Check for leading/trailing spaces in column names\n",
    "df_bank.columns = df_bank.columns.str.strip()\n",
    "\n",
    "# Print columns to debug\n",
    "print(\"Columns in DataFrame:\", df_bank.columns)\n",
    "\n",
    "# Define a function to convert monetary columns to numeric\n",
    "def convert_monetary_column(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        df[column_name] = df[column_name].replace({'\\$': '', ',': ''}, regex=True)\n",
    "        df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
    "\n",
    "# Convert monetary columns\n",
    "convert_monetary_column(df_bank, 'WITHDRAWAL AMT')\n",
    "convert_monetary_column(df_bank, 'DEPOSIT AMT')\n",
    "convert_monetary_column(df_bank, 'BALANCE AMT')\n",
    "\n",
    "# Handle missing values\n",
    "df_bank['WITHDRAWAL AMT'].fillna(0, inplace=True)\n",
    "df_bank['DEPOSIT AMT'].fillna(0, inplace=True)\n",
    "df_bank['BALANCE AMT'].fillna(df_bank['BALANCE AMT'].mean(), inplace=True)\n",
    "\n",
    "# Define the function to classify fraud types\n",
    "def classify_fraud(row):\n",
    "    if row['WITHDRAWAL AMT'] > 1000 and row['DEPOSIT AMT'] == 0:\n",
    "        return 'HighValueTransactions'\n",
    "    elif row['WITHDRAWAL AMT'] < 10 and row['DEPOSIT AMT'] < 10:\n",
    "        return 'LowValueTransactions'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply the classify_fraud function\n",
    "df_bank['FRAUD_TYPE'] = df_bank.apply(classify_fraud, axis=1)\n",
    "\n",
    "# Verify if 'FRAUD_TYPE' has been added\n",
    "print(df_bank[['FRAUD_TYPE']].head())\n",
    "\n",
    "# Check if 'FRAUD_TYPE' column exists\n",
    "if 'FRAUD_TYPE' not in df_bank.columns:\n",
    "    raise ValueError(\"The 'FRAUD_TYPE' column is missing from the DataFrame.\")\n",
    "\n",
    "# Filter out rows with 'Unknown' fraud type\n",
    "df_fraud = df_bank[df_bank['FRAUD_TYPE'] != 'Unknown']\n",
    "X_fraud = df_fraud.drop(columns=['FRAUD_TYPE'])\n",
    "y_fraud = df_fraud['FRAUD_TYPE']\n",
    "\n",
    "# Ensure all feature columns are numeric\n",
    "X_fraud = X_fraud.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill missing values if any\n",
    "X_fraud.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_fraud_scaled = scaler.fit_transform(X_fraud)\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_fraud_encoded = label_encoder.fit_transform(y_fraud)\n",
    "y_fraud_encoded = to_categorical(y_fraud_encoded)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
    "    X_fraud_scaled, y_fraud_encoded, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train_fraud.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train_fraud.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_fraud, y_train_fraud,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = model.predict(X_test_fraud)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_test = np.argmax(y_test_fraud, axis=1)\n",
    "\n",
    "print(\"Model evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gradient Boost to classify the Fraud Patterns\n",
    "\n",
    "Gradient Boosting gives more weight to the harder-to-classify cases, such as fraud patterns, which enhances accuracy, while also reducing the risk of overfitting by combining multiple weaker models to create a robust and effective final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load data\n",
    "bank_data = pd.read_csv('/content/bank_transactions.csv')\n",
    "creditcard_data = pd.read_csv('/content/creditcard.csv')\n",
    "\n",
    "# Remove extra spaces from column names\n",
    "bank_data.columns = bank_data.columns.str.strip()\n",
    "creditcard_data.columns = creditcard_data.columns.str.strip()\n",
    "\n",
    "# Ensure 'fraud_type' column is present in the credit card data\n",
    "if 'Fraud_Type' not in creditcard_data.columns:\n",
    "    raise KeyError(\"The 'fraud_type' column is missing from the creditcard_data.\")\n",
    "\n",
    "# Convert relevant columns to numeric, coercing errors to NaN\n",
    "numeric_columns = ['WITHDRAWAL AMT', 'DEPOSIT AMT', 'BALANCE AMT']\n",
    "for col in numeric_columns:\n",
    "    bank_data[col] = pd.to_numeric(bank_data[col], errors='coerce')\n",
    "    creditcard_data[col] = pd.to_numeric(creditcard_data[col], errors='coerce')\n",
    "\n",
    "# Fill missing values with 0\n",
    "bank_data[numeric_columns] = bank_data[numeric_columns].fillna(0)\n",
    "creditcard_data[numeric_columns] = creditcard_data[numeric_columns].fillna(0)\n",
    "\n",
    "# Prepare features and target variables\n",
    "features = ['WITHDRAWAL AMT', 'DEPOSIT AMT', 'BALANCE AMT']\n",
    "X_bank = bank_data[features]\n",
    "y_bank = bank_data['FRAUD_TYPE']\n",
    "X_creditcard = creditcard_data[features]\n",
    "y_creditcard = creditcard_data['Fraud_Type']\n",
    "\n",
    "# Combine features and target variables from both datasets\n",
    "X_combined = pd.concat([X_bank, X_creditcard], axis=0)\n",
    "y_combined = pd.concat([y_bank, y_creditcard], axis=0)\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_combined)\n",
    "\n",
    "# Split the data while maintaining class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Resample the training data using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize and fit the scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build Gradient Boosting model\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load and prepare your data\n",
    "# Assuming X and y are already defined\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Train set class distribution:\\n\", y_train.value_counts())\n",
    "print(\"Test set class distribution:\\n\", y_test.value_counts())\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Resampled train set class distribution:\\n\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking the Models Together\n",
    "\n",
    "By stacking models together, I combine their strengths and leverage their diverse perspectives to enhance overall performance, resulting in more accurate and robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preparing the data\n",
    "# Create a example data set\n",
    "num_samples = 1000\n",
    "num_features = 20\n",
    "X = np.random.rand(num_samples, num_features)\n",
    "y = np.random.randint(0, 2, num_samples)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Defining the base models\n",
    "# I've selected the random forest model and the neural network\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "neural_network = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100, random_state=42)\n",
    "\n",
    "# Defining and creating the stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', random_forest),\n",
    "        ('nn', neural_network)\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(n_estimators=50, random_state=42)  \n",
    ")\n",
    "\n",
    "# Define a pipeline that includes scaling and stacking\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('stacking', stacking_model)\n",
    "])\n",
    "\n",
    "# Training and evaluating the model\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertuning The Stacked Model\n",
    "\n",
    "I hyper-tuned the stacked model to optimize its performance by finding the best combination of parameters that maximizes its accuracy and effectiveness in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampled the non-fraudulent class and shuffled the data to prevent data leakage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/content/creditcard.csv')\n",
    "\n",
    "# Create a smaller dataset instead of 240K rows, we'll look at 25K\n",
    "data_small = data.sample(n=25000, random_state=42)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data_small.drop(columns=['Class'])\n",
    "y = data_small['Class']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_estimators = [\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('nn', MLPClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Define a pipeline with preprocessing and stacking\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('stacking', stacking_model)\n",
    "])\n",
    "\n",
    "# Defining a parameter grid for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'stacking__rf__n_estimators': randint(50, 201),\n",
    "    'stacking__rf__max_depth': [None, 10, 20, 30, 40],\n",
    "    'stacking__nn__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'stacking__nn__activation': ['tanh', 'relu'],\n",
    "    'stacking__final_estimator__C': uniform(0.1, 10),\n",
    "    'stacking__final_estimator__penalty': ['l2']\n",
    "}\n",
    "\n",
    "# Setting up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=30, cv=3, n_jobs=-1, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\\n\", random_search.best_params_)\n",
    "print(\"Best Score:\\n\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve and AUC Score of the Stacked Ensemble Model\n",
    "\n",
    "I used ROC AUC for performance testing because it provides a comprehensive measure of a modelâ€™s ability to distinguish between classes by evaluating its true positive rate against its false positive rate across different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Get predicted probabilities for the positive class (1) from the test set\n",
    "y_prob = random_search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "# Optional: Plot the ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, marker='o', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
